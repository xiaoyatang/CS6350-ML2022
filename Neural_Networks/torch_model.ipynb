{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch import optim as optim #sgd,...\n",
    "from torch.nn import functional as F  #ReLu, tanh....\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.autograd import Variable \n",
    "# from dataset import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import datasets as datasets\n",
    "from torchvision import transforms as transforms\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varX_train=Variable(torch.from_numpy(X_train))  #turn numpy to variable\n",
    "# varY_train=Variable(torch.from_numpy(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.read_csv(\"./data/train.csv\", header = None,names=['x1','x2','x3','x4','y'])\n",
    "testData = pd.read_csv(\"./data/test.csv\", header = None,names=['x1','x2','x3','x4','y'])\n",
    "X_train= np.array(trainData.iloc[:,0:trainData.shape[1]-1].values)\n",
    "y_train= np.array(trainData.iloc[:,-1].values)[:,None]\n",
    "y_train[y_train==0]=-1\n",
    "X_test= np.array(testData.iloc[:,0:testData.shape[1]-1].values)\n",
    "y_test= np.array(testData.iloc[:,-1].values)[:,None]\n",
    "y_test[y_test==0]=-1\n",
    "minn=X_train.min(axis=0,keepdims=True)\n",
    "maxx=X_train.max(axis=0,keepdims=True)\n",
    "meann=X_train.mean(axis=0,keepdims=True)\n",
    "X_train=(X_train-minn)/(maxx-minn)\n",
    "X_test=(X_test-minn)/(maxx-minn)\n",
    "X_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BankNote(Dataset):\n",
    "    def __init__(self, X,y):\n",
    "        super(BankNote, self).__init__()\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.X[idx,:], self.y[idx,:]\n",
    "    \n",
    "    def __len__(self,):\n",
    "        # Return total number of samples.\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = BankNote(X_train,y_train)\n",
    "dataset_test = BankNote(X_test,y_test)\n",
    "dataset_val = BankNote(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, config, act=nn.Tanh(),init=torch.nn.init.xavier_uniform):\n",
    "                # hidden_dim=100, outputDim=1, weight_scale=1e-3, reg=0.0):     #weight_scale is the variation of normal dis. reg is for regularizer. input_dim is the dim before augmentation.\n",
    "        super(Net, self).__init__()  #super can implicitly connect method of subclass to method of superclass(父类)\n",
    "        layers_list = []\n",
    "        self.init=init\n",
    "\n",
    "        for l in range(len(config)-2):\n",
    "          in_dim =  config[l]\n",
    "          out_dim = config[l+1]\n",
    "\n",
    "          layers_list.append(nn.Linear(in_features=in_dim, out_features=out_dim))\n",
    "          layers_list.append(act)\n",
    "        #last layer\n",
    "        layers_list.append(nn.Linear(in_features=config[-2], out_features=config[-1]))\n",
    "        self.net = nn.Sequential(*layers_list)\n",
    "        self.net.apply(self.init_weights)\n",
    "    def init_weights(self,m):\n",
    "      if isinstance(m, nn.Linear):\n",
    "          self.init(m.weight)\n",
    "          # if self.init==\"xavier\":\n",
    "          #   torch.nn.init.xavier_uniform(m.weight)\n",
    "          # elif self.init==\"he\":\n",
    "          #   torch.nn.init.kaiming_normal(m.weight)\n",
    "          m.bias.data.fill_(0.01)\n",
    "    def forward(self,X):\n",
    "      # h=X\n",
    "      # for layer in self.net:\n",
    "      #   h=layer(h)\n",
    "      return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Solver(object):\n",
    "    def __init__(self, model, data,lossfcn,optimizer, **kwargs):\n",
    "        self.model = model\n",
    "        self.trainData = data['train']\n",
    "        self.valData = data['val']\n",
    "\n",
    "        # if the key exists, pop out the value; if not, use the second value.\n",
    "        self.num_epochs = kwargs.pop('num_epochs', 10)   #numbers of using the whole dataset\n",
    "\n",
    "     \n",
    "        self.print_every = kwargs.pop('print_every', 1)\n",
    "        self.verbose = kwargs.pop('verbose', True)\n",
    "        self.lossfcn=lossfcn\n",
    "        self.optimizer=optimizer\n",
    "        self.train_loss_history=[]\n",
    "        self.train_acc_historyEpoch=[]\n",
    "        self.val_acc_historyEpoch=[]\n",
    "        self.best_val_acc=0\n",
    "    def epochTrain(self):\n",
    "        # Make a minibatch of training data\n",
    "        for idx,(data_x,data_y) in enumerate(self.trainData):\n",
    "            print(data_x.shape,data_y.shape)\n",
    "            # calls hooks like this one\n",
    "            # on_train_batch_start()\n",
    "\n",
    "            # train step\n",
    "            y_pred=self.model(data_x)\n",
    "\n",
    "            loss = self.lossfcn(y_pred,data_y)\n",
    "\n",
    "            # clear gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "\n",
    "            # update parameters\n",
    "            self.optimizer.step()\n",
    "        self.loss_history.append(loss)\n",
    "    def check_accuracy(self,testDataLoader):\n",
    "        y_pred=[]\n",
    "        y_true=[]\n",
    "        for idx,(BatchData_x,BatchData_y) in enumerate(testDataLoader):\n",
    "            y_predBatch=self.model(BatchData_x)\n",
    "            y_pred.append(y_predBatch[:,0])\n",
    "            y_true.append(BatchData_y[:,0])\n",
    "        y_pred = np.hstack(y_pred)[:,None]\n",
    "        y_pred[y_pred<0]=-1\n",
    "        y_pred[y_pred>=0]=1\n",
    "        y_true = np.hstack(y_true)[:,None]\n",
    "        acc = np.mean(y_pred == y_true)\n",
    "        # print(acc,'acc')\n",
    "        return acc\n",
    "\n",
    "    def train(self,epochs):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.epochTrain()\n",
    "            \n",
    "            # if t % self.print_every == 0:\n",
    "            print('(epoch %d / %d) loss: %f' % (\n",
    "                    epochs, epochs, self.loss_history[-1]))\n",
    "            # print(self.optim_configs['learning_rate'])\n",
    "\n",
    "            train_acc = self.check_accuracy(self.X_train, self.y_train)\n",
    "            val_acc = self.check_accuracy(self.X_val, self.y_val)\n",
    "            self.train_acc_historyEpoch.append(train_acc)\n",
    "            self.val_acc_historyEpoch.append(val_acc)\n",
    "            \n",
    "            # Keep track of the best model\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                self.best_params = {}\n",
    "                for k, v in self.model.params.items():\n",
    "                    self.best_params[k] = v.copy()\n",
    "\n",
    "        # At the end of training swap the best params into the model\n",
    "        self.model.params = self.best_params\n",
    "        # print(self.loss_history)\n",
    "        # self.model.params = self.best_params\n",
    "        return self.loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2r/43bc0vp96j18w7w_r9y2brth0000gn/T/ipykernel_73379/3439301798.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  self.init(m.weight)\n"
     ]
    }
   ],
   "source": [
    "config = [4, 64, 64, 1]\n",
    "\n",
    "model = Net(config)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 4]) torch.Size([64, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m loss_func\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     12\u001b[0m solver\u001b[38;5;241m=\u001b[39mSolver(model\u001b[38;5;241m=\u001b[39mmodel,data\u001b[38;5;241m=\u001b[39mdata,lossfcn\u001b[38;5;241m=\u001b[39mloss_func,optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[0;32m---> 13\u001b[0m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [7], line 58\u001b[0m, in \u001b[0;36mSolver.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m,epochs):\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;66;03m# if t % self.print_every == 0:\u001b[39;00m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(epoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m     62\u001b[0m                 epochs, epochs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n",
      "Cell \u001b[0;32mIn [7], line 27\u001b[0m, in \u001b[0;36mSolver.epochTrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_x\u001b[38;5;241m.\u001b[39mshape,data_y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# calls hooks like this one\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# on_train_batch_start()\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# train step\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m y_pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlossfcn(y_pred,data_y)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# clear gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [6], line 30\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[1;32m     27\u001b[0m   \u001b[38;5;66;03m# h=X\u001b[39;00m\n\u001b[1;32m     28\u001b[0m   \u001b[38;5;66;03m# for layer in self.net:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m   \u001b[38;5;66;03m#   h=layer(h)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset_train, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=64, shuffle=False)\n",
    "\n",
    "epochs=5000\n",
    "lr=1e-3\n",
    "reg=1e-5\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=reg)\n",
    "data={\"train\":train_loader,\"val\":val_loader}\n",
    "loss_func=nn.MSELoss()\n",
    "solver=Solver(model=model,data=data,lossfcn=loss_func,optimizer=optimizer)\n",
    "solver.train(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset_train, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=64, shuffle=False)\n",
    "\n",
    "epochs=5000\n",
    "lr=1e-3\n",
    "reg=1e-5\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=reg)\n",
    "\n",
    "hist_rmse_tr = []\n",
    "hist_rmse_te = []\n",
    "hist_pred_te = []\n",
    "\n",
    "for i in tqdm(range(epochs+1)):\n",
    "    for Xtr, ytr in train_loader:\n",
    "    # Xtr, ytr = next(iter(train_loader))\n",
    "    # Xte, yte = next(iter(test_loader))\n",
    "    \n",
    "    # Xtr, ytr, Xte, yte = \\\n",
    "    #     Xtr.float(), ytr.float(), Xte.float(), yte.float()\n",
    "    \n",
    "        pred = model(Xtr)\n",
    "        loss = torch.mean(torch.square(pred-ytr))\n",
    "        \n",
    "\n",
    "        # step 1: clear the grads\n",
    "        optimizer.zero_grad()\n",
    "        # step 2: backward the computational graph\n",
    "        loss.backward()\n",
    "        # step 3: take the gradient step\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i%500 == 0:\n",
    "            print('r', 'Epoch #{}\\t: '.format(i), end='')\n",
    "            with torch.no_grad():\n",
    "                for Xtr, ytr in val_loader:\n",
    "                    rmse_tr = torch.sqrt(torch.mean(torch.square(ytr-pred)))\n",
    "                    rmse_te = torch.sqrt(torch.mean(torch.square(yte-model(Xte))))\n",
    "                    print('train_rmse={:.5f}, test_rmse={:.5f}'.format(rmse_tr.item(), rmse_te.item()))\n",
    "                    \n",
    "                    hist_rmse_tr.append(rmse_tr.item())\n",
    "                    hist_rmse_te.append(rmse_te.item())\n",
    "                    hist_pred_te.append(model(Xte).data.cpu().numpy())\n",
    "        #\n",
    "    #\n",
    "\n",
    "\n",
    "hist_rmse_tr = np.array(hist_rmse_tr)\n",
    "hist_rmse_te = np.array(hist_rmse_te)\n",
    "hist_pred_te = np.array(hist_pred_te)\n",
    "\n",
    "# You can saved the \n",
    "# create_path('__chkp__')\n",
    "# torch.save(model.state_dict(), os.path.join('__chkp__', 'pretrained.dict')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fdas\n"
     ]
    }
   ],
   "source": [
    "print(\"fdas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([[1.0, -1.0],\n",
    "                  [0.0,  1.0],\n",
    "                  [0.0,  0.0]])\n",
    "\n",
    "in_features = x.shape[1]  # = 2\n",
    "out_features = 2\n",
    "\n",
    "m = nn.Linear(in_features, out_features)\n",
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5486a802d94f70a4c28ef8aae5c17580948097a99489b4df271cbee1d7448fa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
