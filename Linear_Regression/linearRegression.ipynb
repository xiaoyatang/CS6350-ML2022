{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from numpy import linalg as la\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/train.csv\", header = None,names=['x1','x2','x3','x4','x5','x6','x7','y'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocess\n",
    "data.insert(0,'ones',1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training preprocess\n",
    "cols=data.shape[1]\n",
    "x = data.iloc[:,0:cols-1]\n",
    "y = data.iloc[:,cols-1:cols]\n",
    "x = np.matrix(x.values)\n",
    "y = np.matrix(y.values)\n",
    "# theta=np.matrix(np.array([0,0,0,0]))\n",
    "print(x.shape,y.shape,len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define cost \n",
    "def computeCost(x,y,theta):\n",
    "    innerCost=np.power((y-x * theta.T),2)\n",
    "    sumCost=(np.sum(innerCost)/(2*len(x)))\n",
    "    return sumCost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute gradient\n",
    "def batGradient(x,y,theta):\n",
    "    parameters = 8\n",
    "    grad=np.zeros(parameters)\n",
    "    error = y-x * theta.T\n",
    "    for j in range(parameters):\n",
    "            grad[j] = -np.sum(np.multiply(error,x[:,j]))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta=np.matrix(np.zeros(8))\n",
    "# grad = batGradient(x,y,theta)\n",
    "# print(grad,'grad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch gradient descent\n",
    "def batGradientDescent(x,y):\n",
    "    iterFinal = 1\n",
    "    alpha = 1\n",
    "    para=8\n",
    "\n",
    "    while alpha>1e-3:\n",
    "        # theta =np.matrix(np.zeros((10001,para)))\n",
    "        theta = np.matrix(np.zeros(para))\n",
    "        temp=np.matrix(np.zeros(para))\n",
    "        # print(theta,'theta')\n",
    "        cost=[]\n",
    "        for iters in range(10000):\n",
    "            # temp = theta\n",
    "            gradient=batGradient(x,y,theta) #gradient is a array[j].\n",
    "            # print(gradient,'gradient')\n",
    "            for j in range(para):\n",
    "                temp[0,j]=theta[0,j]\n",
    "                theta[0,j]=theta[0,j]-alpha*gradient[j] #theta[0,j] store w_t+1\n",
    "\n",
    "            # print(alpha*gradient,'delta')  \n",
    "            # print(temp,'temp')\n",
    "            # print(theta,'theta')\n",
    "            cost.append(computeCost(x,y,theta))\n",
    "            if np.linalg.norm(theta-temp) < 0.000001:\n",
    "                iterFinal = iters+1\n",
    "                costFinal=computeCost(x,y,theta)\n",
    "                print(costFinal)\n",
    "                print(theta-temp,'theta-temp')\n",
    "                # print(iterFinal,theta,alpha)\n",
    "                return [iterFinal,theta,alpha,cost] \n",
    "        costCur=computeCost(x,y,theta)\n",
    "        print(costCur,\"costCur\")\n",
    "        # print(computeCost(x,y,theta),\"split\")\n",
    "        # iters=iters+1\n",
    "        alpha *= 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterFinal,theta,alpha,cost=batGradientDescent(x,y)\n",
    "print(iterFinal,theta,alpha,cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(len(cost)), cost, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import test data\n",
    "data1= pd.read_csv(\"./data/test.csv\", header = None,names=['x1','x2','x3','x4','x5','x6','x7','y'])\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the test data\n",
    "data1.insert(0,'ones',1)\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the test data\n",
    "cols=data1.shape[1]\n",
    "x1 = data1.iloc[:,0:cols-1]\n",
    "y1 = data1.iloc[:,cols-1:cols]\n",
    "x1 = np.matrix(x1.values)\n",
    "y1 = np.matrix(y1.values)\n",
    "# theta=np.matrix(np.array([0,0,0,0]))\n",
    "print(x1.shape,y1.shape,len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the cost\n",
    "testCost=computeCost(x1,y1,theta)\n",
    "print(theta,testCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the analytical solution\n",
    "X=np.linalg.pinv(x.T*x)\n",
    "optTheta=X*x.T*y\n",
    "print(optTheta[:,0])\n",
    "print(np.squeeze(optTheta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic gradient\n",
    "def stoGradientDescent(x,y,theta,alpha,i):\n",
    "    temp=theta\n",
    "    parameters = 4\n",
    "    cost=np.zeros(i)\n",
    "    grad=np.zeros(parameters)\n",
    "    error = y-x * theta.T\n",
    "    for j in range(parameters):\n",
    "        grad[j] = error[i] * x[i,j]\n",
    "        temp[0,j]=theta[0,j]-alpha * grad[j]\n",
    "    print(grad,'grad')\n",
    "    theta=temp\n",
    "    cost=computeCost(x,y,theta)\n",
    "    return cost,theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumSamples=x.shape[0]\n",
    "SampleX=np.random.choice(NumSamples)\n",
    "print(SampleX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "NumSamples=x.shape[0]\n",
    "def stoGradientDescentBatch(x,y):\n",
    "    iterFinal = 1\n",
    "    alpha = 1\n",
    "    para=8\n",
    "    costMin=10000\n",
    "    bestWeight=None\n",
    "    while alpha>1e-3:\n",
    "        # theta =np.matrix(np.zeros((10001,para)))\n",
    "        theta = np.matrix(np.zeros(para))\n",
    "        temp=np.matrix(np.zeros(para))\n",
    "        # print(theta,'theta')\n",
    "        cost=[]\n",
    "\n",
    "        for iters in range(30000):\n",
    "            # temp = theta\n",
    "            SampleIndex=np.random.choice(NumSamples)\n",
    "            gradient=batGradient(x[SampleIndex,:],y[SampleIndex,:],theta) #gradient is a array[j].\n",
    "            # print(gradient,'gradient')\n",
    "            for j in range(para):\n",
    "                temp[0,j]=theta[0,j]\n",
    "                theta[0,j]=theta[0,j]-alpha*gradient[j] #theta[0,j] store w_t+1\n",
    "\n",
    "            # print(alpha*gradient,'delta')  \n",
    "            # print(temp,'temp')\n",
    "            # print(theta,'theta')\n",
    "            CostCur=computeCost(x,y,theta)\n",
    "            cost.append(CostCur)\n",
    "            if CostCur<costMin:\n",
    "                bestWeight=theta\n",
    "            if np.linalg.norm(theta-temp) < 0.000001:\n",
    "                iterFinal = iters+1\n",
    "                costFinal=computeCost(x,y,theta)\n",
    "                print(costFinal)\n",
    "                print(theta-temp,'theta-temp')\n",
    "                # print(iterFinal,theta,alpha)\n",
    "                return [iterFinal,theta,alpha,cost,bestWeight] \n",
    "        costCur=computeCost(x,y,theta)\n",
    "        print(costCur,\"costCur\")\n",
    "        # print(computeCost(x,y,theta),\"split\")\n",
    "        # iters=iters+1\n",
    "        alpha *= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterFinal,theta,alpha,cost,bestWeight=stoGradientDescentBatch(x,y)\n",
    "print(iterFinal,theta,alpha,cost,bestWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(computeCost(x,y,bestWeight))\n",
    "print(iterFinal,theta,alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(len(cost)), cost, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha=0.5\n",
    "# iters=5000\n",
    "# theta = np.matrix(np.zeros(8))\n",
    "# costFinal,theta=batGradientDescent(x,y,theta,alpha,iters)\n",
    "# print(costFinal,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "\n",
    "# if sys.argv[1] == \"bgd\":\n",
    "#         [itersFinal,theta, alpha] = batGradientDescent(x,y)\n",
    "        #   print('iterations:',iters)\n",
    "#         print(\"W: \", theta)\n",
    "#         print(\"R: \", alpha)\n",
    "#         print(\"TEST COST: \", computeCost(x,y,theta))\n",
    "# elif sys.argv[1] == \"sgd\":\n",
    "#         [w, r] = stoGradientDescent()\n",
    "#         print(\"W: \", w)\n",
    "#         print(\"R: \", r)\n",
    "#         print(\"TEST COST: \", computeCost(x,y,theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5486a802d94f70a4c28ef8aae5c17580948097a99489b4df271cbee1d7448fa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
